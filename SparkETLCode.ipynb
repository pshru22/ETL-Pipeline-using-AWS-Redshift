{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Code for ETL Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting all the necessary env variables for initiating Spark session\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-96.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL Assignment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f54f86361d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ETL Assignment').master(\"local\").getOrCreate()\n",
    "spark\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using StructType to define schema for the input file\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType, LongType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying the schema as per the data dictionary\n",
    "fileSchema = StructType([StructField('year',IntegerType(),True),\n",
    "                        StructField('month',StringType(),True),\n",
    "                        StructField('day',IntegerType(),True),\n",
    "                        StructField('weekday',StringType(),True),\n",
    "                        StructField('hour',IntegerType(),True),\n",
    "                        StructField('atm_status',StringType(),True),\n",
    "                        StructField('atm_id',StringType(),True),\n",
    "                        StructField('atm_manufacturer',StringType(),True),\n",
    "                        StructField('atm_location',StringType(),True),\n",
    "                        StructField('atm_streetname',StringType(),True),\n",
    "                        StructField('atm_street_number',IntegerType(),True),\n",
    "                        StructField('atm_zipcode',IntegerType(),True),\n",
    "                        StructField('atm_lat',FloatType(),True),\n",
    "                        StructField('atm_lon',FloatType(),True),\n",
    "                        StructField('currency',StringType(),True),\n",
    "                        StructField('card_type',StringType(),True), \n",
    "                        StructField('transaction_amount', IntegerType(),True),\n",
    "                        StructField('service',StringType(),True),\n",
    "                        StructField('message_code',StringType(),True),\n",
    "                        StructField('message_text',StringType(),True),\n",
    "                        StructField('weather_lat',FloatType(),True),\n",
    "                        StructField('weather_lon',FloatType(),True),\n",
    "                        StructField('weather_city_id',IntegerType(),True),\n",
    "                        StructField('weather_city_name',StringType(),True), \n",
    "                        StructField('temp',FloatType(),True),\n",
    "                        StructField('pressure',IntegerType(),True),\n",
    "                        StructField('humidity',IntegerType(),True),\n",
    "                        StructField('wind_speed', IntegerType(),True),\n",
    "                        StructField('wind_deg',IntegerType(),True),\n",
    "                        StructField('rain_3h',FloatType(),True),\n",
    "                        StructField('clouds_all',IntegerType(),True),\n",
    "                        StructField('weather_id',IntegerType(),True),\n",
    "                        StructField('weather_main',StringType(),True),\n",
    "                        StructField('weather_description',StringType(),True),\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the imported input dataset in Hadoop. JHeader is false as the file does not have headers\n",
    "inp_ds= spark.read.csv(\"hdfs:/user/root/Atm_data/part-m-00000\", header = False, schema = fileSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: float (nullable = true)\n",
      " |-- atm_lon: float (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: float (nullable = true)\n",
      " |-- weather_lon: float (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: float (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: float (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the input file schema\n",
    "inp_ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+--------------------+\n",
      "|year|  month|day|weekday|hour|atm_status|atm_id|atm_manufacturer|atm_location|     atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|currency| card_type|transaction_amount|   service|message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|  temp|pressure|humidity|wind_speed|wind_deg|rain_3h|clouds_all|weather_id|weather_main| weather_description|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+--------------------+\n",
      "|2017|January|  1| Sunday|   0|    Active|     1|             NCR|  NÃƒÂ¦stved|        Farimagsvej|                8|       4700| 55.233| 11.763|     DKK|MasterCard|              5643|Withdrawal|        null|        null|      55.23|     11.761|        2616038|         Naestved|281.15|    1014|      87|         7|     260|  0.215|        92|       500|        Rain|          light rain|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     2|             NCR|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|     DKK|MasterCard|              1764|Withdrawal|        null|        null|     57.048|      9.935|        2616235|   NÃƒÂ¸rresundby|280.64|    1020|      93|         9|     250|   0.59|        92|       500|        Rain|          light rain|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     2|             NCR|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|     DKK|      VISA|              1891|Withdrawal|        null|        null|     57.048|      9.935|        2616235|   NÃƒÂ¸rresundby|280.64|    1020|      93|         9|     250|   0.59|        92|       500|        Rain|          light rain|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     3|             NCR|       Ikast|RÃƒÂ¥dhusstrÃƒÂ¦det|               12|       7430| 56.139|  9.154|     DKK|      VISA|              4166|Withdrawal|        null|        null|     56.139|      9.158|        2619426|            Ikast|281.15|    1011|     100|         6|     240|    0.0|        75|       300|     Drizzle|light intensity d...|\n",
      "|2017|January|  1| Sunday|   0|    Active|     4|             NCR|  Svogerslev|       BrÃƒÂ¸nsager|                1|       4000| 55.634| 12.018|     DKK|MasterCard|              5153|Withdrawal|        null|        null|     55.642|      12.08|        2614481|         Roskilde|280.61|    1014|      87|         7|     260|    0.0|        88|       701|        Mist|                mist|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the top 5 rows \n",
    "inp_ds.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying the count of the inp_ds\n",
    "inp_ds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### as seen above, the inp_ds has 2468572 records which is equal to the number of records imported from RDS to HDFS via sqoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building dimension tables using spark dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) DIM_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: float (nullable = true)\n",
      " |-- atm_lon: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#selecting required columns from source and removing duplicates\n",
    "df_0=inp_ds.select('atm_location','atm_streetname','atm_street_number','atm_zipcode','atm_lat','atm_lon').distinct()\n",
    "df_0.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we have 109 records in DIM_LOC table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: float (nullable = true)\n",
      " |-- atm_lon: float (nullable = true)\n",
      " |-- location_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# genearting surrogate key\n",
    "df_0_schema = df_0.withColumn(\"location_id\", lit(1))\n",
    "df_0_schema.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting df into RDD to generate SK's\n",
    "rdd_1 = df_0.rdd.zipWithIndex().map(lambda (row,rowId): ( list(row) + [rowId+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: float (nullable = true)\n",
      " |-- atm_lon: float (nullable = true)\n",
      " |-- location_id: integer (nullable = false)\n",
      "\n",
      "+--------------+--------------------+-----------------+-----------+-------+-------+-----------+\n",
      "|  atm_location|      atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|location_id|\n",
      "+--------------+--------------------+-----------------+-----------+-------+-------+-----------+\n",
      "|   SÃƒÂ¦by Syd|Trafikcenter SÃƒÂ...|                1|       9300| 57.313|  10.45|          1|\n",
      "|   GlyngÃƒÂ¸re|         FÃƒÂ¦rgevej|                1|       7870| 56.762|  8.867|          2|\n",
      "|Skelagervej 15|         Skelagervej|               15|       9000| 57.023|  9.891|          3|\n",
      "|         Durup|              Torvet|                4|       7870| 56.745|  8.949|          4|\n",
      "|     Svendborg|  Sankt Nicolai Gade|                1|       5700| 55.058| 10.609|          5|\n",
      "+--------------+--------------------+-----------------+-----------+-------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert RDD to DF using defined schema\n",
    "\n",
    "rdd_to_df0 = spark.createDataFrame(rdd_1, schema=df_0_schema.schema)\n",
    "rdd_to_df0.printSchema()\n",
    "rdd_to_df0.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange columns as per target def\n",
    "df0_rearg=rdd_to_df0.select(\"location_id\",\"atm_location\",\"atm_streetname\",\"atm_street_number\",\"atm_zipcode\", \n",
    "\"atm_lat\",\"atm_lon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+-----------------+-----------+-------+-------+\n",
      "|location_id|  atm_location|      atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|\n",
      "+-----------+--------------+--------------------+-----------------+-----------+-------+-------+\n",
      "|          1|   SÃƒÂ¦by Syd|Trafikcenter SÃƒÂ...|                1|       9300| 57.313|  10.45|\n",
      "|          2|   GlyngÃƒÂ¸re|         FÃƒÂ¦rgevej|                1|       7870| 56.762|  8.867|\n",
      "|          3|Skelagervej 15|         Skelagervej|               15|       9000| 57.023|  9.891|\n",
      "|          4|         Durup|              Torvet|                4|       7870| 56.745|  8.949|\n",
      "|          5|     Svendborg|  Sankt Nicolai Gade|                1|       5700| 55.058| 10.609|\n",
      "+-----------+--------------+--------------------+-----------------+-----------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0_rearg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final dim_loc df\n",
    "dim_loc= df0_rearg.withColumnRenamed(\"atm_location\",\"location\").withColumnRenamed(\"atm_streetname\",\"streetname\").withColumnRenamed(\"atm_street_number\",\"street_number\").withColumnRenamed(\"atm_zipcode\",\"zipcode\").withColumnRenamed(\"atm_lat\",\"lat\").withColumnRenamed(\"atm_lon\",\"lon\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+-------------+-------+------+------+\n",
      "|location_id|      location|          streetname|street_number|zipcode|   lat|   lon|\n",
      "+-----------+--------------+--------------------+-------------+-------+------+------+\n",
      "|          1|   SÃƒÂ¦by Syd|Trafikcenter SÃƒÂ...|            1|   9300|57.313| 10.45|\n",
      "|          2|   GlyngÃƒÂ¸re|         FÃƒÂ¦rgevej|            1|   7870|56.762| 8.867|\n",
      "|          3|Skelagervej 15|         Skelagervej|           15|   9000|57.023| 9.891|\n",
      "|          4|         Durup|              Torvet|            4|   7870|56.745| 8.949|\n",
      "|          5|     Svendborg|  Sankt Nicolai Gade|            1|   5700|55.058|10.609|\n",
      "+-----------+--------------+--------------------+-------------+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_loc.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load table to s3 bucket\n",
    "#Setting S3 access key and security key to upload data to S3\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\"AKIAQN3JW5X4EDXA53GE\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\"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\")\n",
    "\n",
    "#S3 (CSV)\n",
    "dim_loc.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"s3a://pshrutis3bucket/etlproj/dim_loc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) DIM_ATM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the location_id from dim_location table, we are joining the input dataset with dim_loc table on latitue and longitude columns\n",
    "\n",
    "joined_df=inp_ds.join(dim_loc,(inp_ds[\"atm_lat\"]==dim_loc[\"lat\"]) & (inp_ds[\"atm_lon\"]==dim_loc[\"lon\"]),\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3398293"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #select direct move src cols and remove duplicates\n",
    "df_2=joined_df.withColumnRenamed(\"atm_id\",\"atm_number\").select('atm_number','atm_manufacturer','location_id').distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- atm_number: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- location_id: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.printSchema()\n",
    "df_2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we have 156 records in the DIM_ATM table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- atm_number: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- location_id: integer (nullable = true)\n",
      " |-- atm_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2_schema = df_2.withColumn(\"atm_id\", lit(1)) #create new SK atm_id\n",
    "df_2_schema.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_2 = df_2.rdd.zipWithIndex().map(lambda (row,rowId): ( list(row) + [rowId+1]))\n",
    "\n",
    "rdd_to_df2 = spark.createDataFrame(rdd_2, schema=df_2_schema.schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearranging and renaming cols\n",
    "#final dim_atm df\n",
    "dim_atm=rdd_to_df2.withColumnRenamed(\"location_id\",\"atm_location_id\").select(\"atm_id\",\"atm_number\",\"atm_manufacturer\",\"atm_location_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+---------------+\n",
      "|atm_id|atm_number|atm_manufacturer|atm_location_id|\n",
      "+------+----------+----------------+---------------+\n",
      "|     1|        74|             NCR|             71|\n",
      "|     2|        33|             NCR|             77|\n",
      "|     3|       101|             NCR|            104|\n",
      "|     4|        50|             NCR|             19|\n",
      "|     5|        36|             NCR|             30|\n",
      "+------+----------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_atm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load table to s3 bucket\n",
    "#Setting S3 access key and security key to upload data to S3\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\"AKIAQN3JW5X4EDXA53GE\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\"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\")\n",
    "\n",
    "#S3 (CSV)\n",
    "dim_atm.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"s3a://pshrutis3bucket/etlproj/dim_atm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) DIM_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting all the reuqired src cols and removing duplicates\n",
    "df_3=inp_ds.select('year','month','day','hour','weekday').distinct()\n",
    "df_3.printSchema()\n",
    "df_3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dim_date has 8685 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#full_date_time is a dervied column \n",
    "\n",
    "df_derv=df_3.withColumn(\"full_date_time\",F.from_unixtime(F.unix_timestamp(F.concat(df_3.year.cast(StringType()),df_3.month.cast(StringType()),F.lpad(df_3.day.cast(StringType()),2,'0'),\n",
    "F.lpad(df_3.hour.cast(StringType()),2,'0')),'yyyyMMMMMddHH'),'YYYY-MM-dd HH:mm:SS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+----+--------+-------------------+\n",
      "|year|  month|day|hour| weekday|     full_date_time|\n",
      "+----+-------+---+----+--------+-------------------+\n",
      "|2017|January|  1|   9|  Sunday|2017-01-01 09:00:00|\n",
      "|2017|January|  3|   5| Tuesday|2017-01-03 05:00:00|\n",
      "|2017|January|  8|  19|  Sunday|2017-01-08 19:00:00|\n",
      "|2017|January| 21|   3|Saturday|2017-01-21 03:00:00|\n",
      "|2017|January| 23|  21|  Monday|2017-01-23 21:00:00|\n",
      "+----+-------+---+----+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_derv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- full_date_time: string (nullable = true)\n",
      " |-- date_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3_schema = df_derv.withColumn(\"date_id\", lit(1)) #generate SK date_id\n",
    "df_3_schema.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+----+--------+-------------------+-------+\n",
      "|year|  month|day|hour| weekday|     full_date_time|date_id|\n",
      "+----+-------+---+----+--------+-------------------+-------+\n",
      "|2017|January|  1|   9|  Sunday|2017-01-01 09:00:00|      1|\n",
      "|2017|January|  3|   5| Tuesday|2017-01-03 05:00:00|      2|\n",
      "|2017|January|  8|  19|  Sunday|2017-01-08 19:00:00|      3|\n",
      "|2017|January| 21|   3|Saturday|2017-01-21 03:00:00|      4|\n",
      "|2017|January| 23|  21|  Monday|2017-01-23 21:00:00|      5|\n",
      "+----+-------+---+----+--------+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_3 = df_derv.rdd.zipWithIndex().map(lambda (row,rowId): ( list(row) + [rowId+1]))\n",
    "rdd_to_df3 = spark.createDataFrame(rdd_3, schema=df_3_schema.schema)\n",
    "rdd_to_df3.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----+-------+---+----+--------+\n",
      "|date_id|     full_date_time|year|  month|day|hour| weekday|\n",
      "+-------+-------------------+----+-------+---+----+--------+\n",
      "|      1|2017-01-01 09:00:00|2017|January|  1|   9|  Sunday|\n",
      "|      2|2017-01-03 05:00:00|2017|January|  3|   5| Tuesday|\n",
      "|      3|2017-01-08 19:00:00|2017|January|  8|  19|  Sunday|\n",
      "|      4|2017-01-21 03:00:00|2017|January| 21|   3|Saturday|\n",
      "|      5|2017-01-23 21:00:00|2017|January| 23|  21|  Monday|\n",
      "+-------+-------------------+----+-------+---+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#genearte final df dim_date by rearranging necessary columns\n",
    "dim_date=rdd_to_df3.select(\"date_id\",\"full_date_time\",\"year\",\"month\",\"day\",\"hour\",\"weekday\")\n",
    "dim_date.count()\n",
    "dim_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load table to s3 bucket\n",
    "#Setting S3 access key and security key to upload data to S3\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\"AKIAQN3JW5X4EDXA53GE\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\"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\")\n",
    "\n",
    "#S3 (CSV)\n",
    "dim_date.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"s3a://pshrutis3bucket/etlproj/dim_date.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) DIM_CARD_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4=inp_ds.select('card_type').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- card_type: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4.printSchema()\n",
    "df_4.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dim_card_type has 12 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- card_type_id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_4_schema = df_4.withColumn(\"card_type_id\", lit(1))\n",
    "df_4_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_4 = df_4.rdd.zipWithIndex().map(lambda (row,rowId): ( list(row) + [rowId+1]))\n",
    "rdd_to_df4 = spark.createDataFrame(rdd_4, schema=df_4_schema.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- card_type_id: integer (nullable = false)\n",
      "\n",
      "+--------------------+------------+\n",
      "|           card_type|card_type_id|\n",
      "+--------------------+------------+\n",
      "|     Dankort - on-us|           1|\n",
      "|              CIRRUS|           2|\n",
      "|         HÃƒÂ¦vekort|           3|\n",
      "|                VISA|           4|\n",
      "|  Mastercard - on-us|           5|\n",
      "|             Maestro|           6|\n",
      "|Visa Dankort - on-us|           7|\n",
      "|        Visa Dankort|           8|\n",
      "|            VisaPlus|           9|\n",
      "|          MasterCard|          10|\n",
      "|             Dankort|          11|\n",
      "| HÃƒÂ¦vekort - on-us|          12|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_to_df4.printSchema()\n",
    "rdd_to_df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_card_type_df final df\n",
    "dim_card_type=rdd_to_df4.select(\"card_type_id\",\"card_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|card_type_id|           card_type|\n",
      "+------------+--------------------+\n",
      "|           1|     Dankort - on-us|\n",
      "|           2|              CIRRUS|\n",
      "|           3|         HÃƒÂ¦vekort|\n",
      "|           4|                VISA|\n",
      "|           5|  Mastercard - on-us|\n",
      "|           6|             Maestro|\n",
      "|           7|Visa Dankort - on-us|\n",
      "|           8|        Visa Dankort|\n",
      "|           9|            VisaPlus|\n",
      "|          10|          MasterCard|\n",
      "|          11|             Dankort|\n",
      "|          12| HÃƒÂ¦vekort - on-us|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_card_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load table to s3 bucket\n",
    "#Setting S3 access key and security key to upload data to S3\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\"AKIAQN3JW5X4EDXA53GE\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\"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\")\n",
    "\n",
    "#S3 (CSV)\n",
    "dim_card_type.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"s3a://pshrutis3bucket/etlproj/dim_card_type.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation of Fact table - join with  Dimension  tables to get corresponding Primary keys from dimension tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* join with dim_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_id: integer (nullable = false)\n",
      " |-- full_date_time: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_date_join1 = inp_ds.join(dim_date,(['year','month','day','hour','weekday']),\"left_outer\") #as the col names are same in both tables,it will retain only single join cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_date_join1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: float (nullable = true)\n",
      " |-- atm_lon: float (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: float (nullable = true)\n",
      " |-- weather_lon: float (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: float (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: float (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- full_date_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_date_join1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* join above result with dim_card_type table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_card_join=dim_date_join1.join(dim_card_type,[\"card_type\"],\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: float (nullable = true)\n",
      " |-- atm_lon: float (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: float (nullable = true)\n",
      " |-- weather_lon: float (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: float (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: float (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- full_date_time: string (nullable = true)\n",
      " |-- card_type_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_card_join.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_card_join.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* join above result with dim_loc table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform join and drop columns to avoid duplicate col names \n",
    "dim_loc_join=dim_card_join.join(dim_loc,(dim_card_join[\"atm_location\"]==dim_loc[\"location\"]) & (dim_card_join[\"atm_lat\"]==dim_loc[\"lat\"]) & (dim_card_join[\"atm_lon\"]==dim_loc[\"lon\"]) & (dim_card_join[\"atm_streetname\"]==dim_loc[\"streetname\"])& (dim_card_join[\"atm_street_number\"]==dim_loc[\"street_number\"])& (dim_card_join[\"atm_zipcode\"]==dim_loc[\"zipcode\"]),\"left_outer\").drop(dim_card_join[\"atm_location\"]).drop(dim_card_join[\"atm_lat\"]).drop(dim_card_join[\"atm_lon\"]).drop(dim_card_join[\"atm_street_number\"]).drop(dim_card_join[\"atm_streetname\"]).drop(dim_card_join[\"atm_zipcode\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_loc_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: float (nullable = true)\n",
      " |-- weather_lon: float (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: float (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: float (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- full_date_time: string (nullable = true)\n",
      " |-- card_type_id: integer (nullable = true)\n",
      " |-- location_id: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- streetname: string (nullable = true)\n",
      " |-- street_number: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lon: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_loc_join.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* join above result set with dim_atm table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with atm_dima nd drop redundant columns\n",
    "dim_atm_join=dim_loc_join.join(dim_atm,(dim_loc_join[\"atm_id\"]==dim_atm[\"atm_number\"]) & (dim_loc_join[\"atm_manufacturer\"]==dim_atm[\"atm_manufacturer\"]) &(dim_loc_join[\"location_id\"]==dim_atm[\"atm_location_id\"]),\"left_outer\").drop(dim_loc_join['atm_id']).drop(dim_loc_join['atm_manufacturer']).drop(dim_atm[\"atm_location_id\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: float (nullable = true)\n",
      " |-- weather_lon: float (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: float (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: float (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- full_date_time: string (nullable = true)\n",
      " |-- card_type_id: integer (nullable = true)\n",
      " |-- location_id: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- streetname: string (nullable = true)\n",
      " |-- street_number: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lon: float (nullable = true)\n",
      " |-- atm_id: integer (nullable = true)\n",
      " |-- atm_number: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_atm_join.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_atm_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select and rename cols as per target table def\n",
    "fact_df=dim_atm_join.withColumnRenamed(\"location_id\",\"weather_loc_id\").select(\"atm_id\",\"weather_loc_id\",\"date_id\",\"card_type_id\",\"atm_status\",\"currency\",\"service\",\"transaction_amount\",\"message_code\",\"message_text\",\"rain_3h\",\"clouds_all\",\"weather_id\",\"weather_main\",\"weather_description\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- atm_id: integer (nullable = true)\n",
      " |-- weather_loc_id: integer (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- card_type_id: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- rain_3h: float (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- atm_id: integer (nullable = true)\n",
      " |-- weather_loc_id: integer (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- card_type_id: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- rain_3h: float (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      " |-- trans_id: integer (nullable = false)\n",
      "\n",
      "root\n",
      " |-- atm_id: integer (nullable = true)\n",
      " |-- weather_loc_id: integer (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- card_type_id: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- rain_3h: float (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      " |-- trans_id: integer (nullable = false)\n",
      "\n",
      "+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+\n",
      "|atm_id|weather_loc_id|date_id|card_type_id|atm_status|currency|   service|transaction_amount|message_code|message_text|rain_3h|clouds_all|weather_id|weather_main| weather_description|trans_id|\n",
      "+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+\n",
      "|     1|            71|     61|           1|  Inactive|     DKK|Withdrawal|              2770|        null|        null|    0.0|         0|       800|       Clear|        Sky is Clear|       1|\n",
      "|     1|            71|    711|           1|  Inactive|     DKK|Withdrawal|              3623|        null|        null|    0.0|        40|       802|      Clouds|    scattered clouds|       2|\n",
      "|     1|            71|   7203|           1|  Inactive|     DKK|Withdrawal|              9009|        null|        null|    0.0|        24|       801|      Clouds|          few clouds|       3|\n",
      "|     1|            71|    883|           1|  Inactive|     DKK|Withdrawal|              5476|        null|        null|    0.0|        90|       804|      Clouds|     overcast clouds|       4|\n",
      "|     1|            71|    883|           1|  Inactive|     DKK|Withdrawal|              3714|        null|        null|    0.0|        90|       804|      Clouds|     overcast clouds|       5|\n",
      "|     1|            71|   4225|           1|  Inactive|     DKK|Withdrawal|               903|        null|        null|    0.0|        36|       802|      Clouds|    scattered clouds|       6|\n",
      "|     1|            71|   6513|           1|  Inactive|     DKK|Withdrawal|               905|        null|        null|    0.0|         0|       800|       Clear|        Sky is Clear|       7|\n",
      "|     1|            71|   4736|           1|  Inactive|     DKK|Withdrawal|              5204|        null|        null|    0.0|        75|       520|        Rain|light intensity s...|       8|\n",
      "|     1|            71|   4677|           1|  Inactive|     DKK|Withdrawal|              3672|        null|        null|    0.0|        56|       803|      Clouds|       broken clouds|       9|\n",
      "|     1|            71|   4677|           1|  Inactive|     DKK|Withdrawal|              9016|        null|        null|    0.0|        56|       803|      Clouds|       broken clouds|      10|\n",
      "+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate trans_id SK\n",
    "\n",
    "df_0=inp_ds.select('atm_location','atm_streetname','atm_street_number','atm_zipcode','atm_lat','atm_lon').distinct()\n",
    "\n",
    "\n",
    "fact_df_schema = fact_df.withColumn(\"trans_id\", lit(1))\n",
    "fact_df_schema.printSchema()\n",
    "rdd_fact = fact_df.rdd.zipWithIndex().map(lambda (row,rowId): ( list(row) + [rowId+1]))\n",
    "\n",
    "rdd_to_fact = spark.createDataFrame(rdd_fact, schema=fact_df_schema.schema)\n",
    "rdd_to_fact.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+-------------------+\n",
      "|trans_id|atm_id|weather_loc_id|date_id|card_type_id|atm_status|currency|   service|transaction_amount|message_code|message_text|rain_3h|clouds_all|weather_id|weather_main|weather_description|\n",
      "+--------+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+-------------------+\n",
      "|       1|     1|            71|     61|           1|  Inactive|     DKK|Withdrawal|              2770|        null|        null|    0.0|         0|       800|       Clear|       Sky is Clear|\n",
      "|       2|     1|            71|    711|           1|  Inactive|     DKK|Withdrawal|              3623|        null|        null|    0.0|        40|       802|      Clouds|   scattered clouds|\n",
      "|       3|     1|            71|   7203|           1|  Inactive|     DKK|Withdrawal|              9009|        null|        null|    0.0|        24|       801|      Clouds|         few clouds|\n",
      "|       4|     1|            71|    883|           1|  Inactive|     DKK|Withdrawal|              5476|        null|        null|    0.0|        90|       804|      Clouds|    overcast clouds|\n",
      "|       5|     1|            71|    883|           1|  Inactive|     DKK|Withdrawal|              3714|        null|        null|    0.0|        90|       804|      Clouds|    overcast clouds|\n",
      "+--------+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rearrange cols for final load\n",
    "fact_dim_load=rdd_to_fact.select(\"trans_id\",\"atm_id\",\"weather_loc_id\",\"date_id\",\"card_type_id\",\"atm_status\",\"currency\",\"service\",\"transaction_amount\",\"message_code\",\"message_text\",\"rain_3h\",\"clouds_all\",\"weather_id\",\"weather_main\",\"weather_description\")\n",
    "\n",
    "\n",
    "\n",
    "fact_dim_load.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_dim_load.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading fact table to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load table to s3 bucket\n",
    "#Setting S3 access key and security key to upload data to S3\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\"AKIAQN3JW5X4EDXA53GE\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\"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\")\n",
    "\n",
    "#S3 (CSV)\n",
    "fact_dim_load.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"s3a://pshrutis3bucket/etlproj/fact_dim_load.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
